[global]
seed = 3123

; ***************  DATASET PARAMS *******************************
;
;
; number of outputs / random numbers to remember
n_outs = 1
; Each sequence will have the length task_base_length
task_T = 30
; If random_pos is chosen, in the first how many elements the values should
; be?
task_inrange = 25
; Range from where to pick random numbers
task_max_val = 1.
task_min_val = 0.
; Number of batches in the train set  
task_train_batches = 2
; batch size for the training set
task_train_batchsize = 300
; batches in validation set
task_valid_batches = 2
task_valid_batchsize = 300
task_test_batches = 4
task_test_batchsize = 400
; wout can be computed using least square closed form formula 
; for this we need an independent set of data on which to apply the 
; formula ( the set should be small enough to fit into memory 
task_wout_batches = 1
task_wout_batchsize = 100
task_noise = 0.
task_wout_noise = 0.008
;
;
; *************** MODEL HYPER-PARAMS ****************************
;
;
; Number of hidden units
nhid = 200
; is the error defined over the entire output or just last step ?
error_over_all = False
; recurrent weight (Whh), input weight (Wux) and output weights (Why)
; initialization. Style can be one of the followings:
;   'orthogonal' -> random orthogonal matrix ( optionally scaled )
;   'random' -> random values of a given scale and density
;   'esn' -> random sparse matrix with a given spectral radius
Whh_style      = 'orthogonal'
Whh_properties = { 'scale' : .2  , 'sparsity' : 0.}
Wux_style      = 'random'
Wux_properties = { 'scale' : .1  , 'sparsity' : 0.3}
Why_style      = 'random'
Why_properties = { 'scale' : .1  , 'sparsity' : 0.3}
; If the regularization should be computed by enforcing :
;   'err' -> the projection of the cost over the product of Jacobians to
;       perserve norm
;   'h[-1]' -> The projection over the vector [1,1,..,1] over the product of
;       Jacobians to perserve norm ( we eliminate dependencies on Wout, norm of
;       the error .. etc.)
;   'random' -> random projections over the product of Jacobians to perserve
;       norm
reg_projection = 'err'
; Should we enforce through the regularization that : 
;   'product' -> the product of Jacobians is an orthonormal matrix
;   'each' -> each Jacobian is an orthonormal matrix
reg_cost = 'each'
; training algorithm : sgd / sgd_qn
opt_alg = 'sgd'
momentum = None
lr_scheme = None 
; [ 2, 20]
alpha_scheme = None
; learning algorithm specific properties
lr       = 0.001
mylambda = 1e-5
t0       = 1e8
skip     = 16
lazy     = 0
; if W_ux should be adapted such to follow the regularization term or not
win_reg = True
; regularization coefficient
alpha = 2.5
; If wout weights should be computed using closed form of linear least square
wout_pinv = True
wiener_lambda = 0.001
; After how many evaluation of the test set to re-adapt the output weights
test_step = 2
;
sum_h = 0
sum_h2 = 0
;
; ********** MAIN TRAINING LOOP HYPER-PARAMS ******************
;
;
; number of epochs
NN = 1e5
; interval used to record error, norm of Jacobians, regularization
small_step = 10
max_storage_numpy = 400
; maximal amount of steps for which to store other statistics (on the test
; set)
max_storage = 30
; if the allocated space for storing statistics is exhaust, overwrite the
; last how many entries ?
go_back = 20
; early-stopping patience / patience_increase
patience_incr = 250
patience = 100

; Path where to store resutls
path = 'stream/'
; '/data/lisatmp/pascanur/RNN_results'
; Name of the result file
name = 'test'
