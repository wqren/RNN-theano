[global]
seed = 3123

; ***************  DATASET PARAMS *******************************
;
;
; number of outputs / random numbers to remember
n_outs = 2
; style of the dataset. You can have :
; 'random_pos' -> values are randomly position and you have a second 
;    channel that is 0 everywhere, except where such a number is 
; 'continous' -> the numbers you need to remember are the first k
;    numbers
style  = 'continous'
; Each sequence will have the length task_base_length
task_base_length = 150
; If random_pos is chosen, in the first how many elements the values should
; be?
task_random_length = 50
; Range from where to pick random numbers
task_max_val = 1.
task_min_val = 0.
; Number of batches in the train set  
task_train_batches = 1000
; batch size for the training set
task_train_batchsize = 20
; batches in validation set
task_valid_batches = 1
task_valid_batchsize = 4000
task_test_batches = 1
task_test_batchsize = 4000
; wout can be computed using least square closed form formula 
; for this we need an independent set of data on which to apply the 
; formula ( the set should be small enough to fit into memory 
task_wout_batches = 100
task_wout_batchsize = 500
task_noise = 0.
task_wout_noise = 0
;
;
; *************** MODEL HYPER-PARAMS ****************************
;
;
; Number of hidden units
nhid = 100
; is the error defined over the entire output or just last step ?
error_over_all = False
; recurrent weight (Whh), input weight (Wux) and output weights (Why)
; initialization. Style can be one of the followings:
;   'orthogonal' -> random orthogonal matrix ( optionally scaled )
;   'random' -> random values of a given scale and density
;   'esn' -> random sparse matrix with a given spectral radius
Whh_style      = 'orthogonal'
Whh_properties = { 'scale' : 1.  , 'sparsity' : 0.}
Wux_style      = 'random'
Wux_properties = { 'scale' : 0.01, 'sparsity' : 0.7}
Why_style      = 'random'
Why_properties = { 'scale' : 1.  , 'sparsity' : 0.7}
; If the regularization should be computed by enforcing :
;   'err' -> the projection of the cost over the product of Jacobians to
;       perserve norm
;   'h[-1]' -> The projection over the vector [1,1,..,1] over the product of
;       Jacobians to perserve norm ( we eliminate dependencies on Wout, norm of
;       the error .. etc.)
;   'random' -> random projections over the product of Jacobians to perserve
;       norm
reg_projection = 'err'
; Should we enforce through the regularization that : 
;   'product' -> the product of Jacobians is an orthonormal matrix
;   'each' -> each Jacobian is an orthonormal matrix
reg_cost = 'product'
; training algorithm : sgd / sgd_qn
opt_alg = 'sgd_qn'
; learning algorithm specific properties
lr       = 0.1
mylambda = 1e-5
t0       = 1e8
skip     = 16
lazy     = True
; if W_ux should be adapted such to follow the regularization term or not
win_reg = False
; regularization coefficient
alpha = 0.1
; If wout weights should be computed using closed form of linear least square
wout_pinv = True
wiener_lambda = 0.001
; After how many evaluation of the test set to re-adapt the output weights
test_step = 2
;
sum_h = 5
sum_h2 = 0
;
; ********** MAIN TRAINING LOOP HYPER-PARAMS ******************
;
;
; number of epochs
NN = 1e6
; interval used to record error, norm of Jacobians, regularization
small_step = 50
; maximal amount of steps for which to store other statistics (on the test
; set)
max_storage = 100
; early-stopping patience / patience_increase
patience_incr = 100
patience = 200
; if the allocated space for storing statistics is exhaust, overwrite the
; last how many entries ?
go_back = 30
; Path where to store resutls
path = '/home/rpascanu/workspace/RNN_theano/results'
; '/data/lisatmp/pascanur/RNN_results'
; Name of the result file
name = 'test'
